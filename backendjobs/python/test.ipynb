{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5ff6b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Teste MySQL-Verbindung...\n",
      "‚úÖ Erfolgreich verbunden mit MySQL Server, Version: 5.5.5-10.5.28-MariaDB-0+deb11u1\n",
      "   Verbunden mit Datenbank: dbjobs\n",
      "\n",
      "üìã Verf√ºgbare Tabellen:\n",
      "   - pma__bookmark\n",
      "   - pma__central_columns\n",
      "   - pma__column_info\n",
      "   - pma__designer_settings\n",
      "   - pma__export_templates\n",
      "   - pma__favorite\n",
      "   - pma__history\n",
      "   - pma__navigationhiding\n",
      "   - pma__pdf_pages\n",
      "   - pma__recent\n",
      "   - pma__relation\n",
      "   - pma__savedsearches\n",
      "   - pma__table_coords\n",
      "   - pma__table_info\n",
      "   - pma__table_uiprefs\n",
      "   - pma__tracking\n",
      "   - pma__userconfig\n",
      "   - pma__usergroups\n",
      "   - pma__users\n",
      "\n",
      "üìä Struktur der Tabelle 'pma__bookmark':\n",
      "   - id (int(10) unsigned)\n",
      "   - dbase (varchar(255))\n",
      "   - user (varchar(255))\n",
      "   - label (varchar(255))\n",
      "   - query (text)\n",
      "\n",
      "üîí MySQL-Verbindung geschlossen.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "MySQL Verbindungstest\n",
    "\n",
    "Dieses Skript testet die Verbindung zu einer MySQL-Datenbank.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import mysql.connector\n",
    "from mysql.connector import Error\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Versuche, .env-Datei zu laden, falls vorhanden\n",
    "load_dotenv()\n",
    "\n",
    "# Datenbank-Konfiguration aus Umgebungsvariablen oder direkt definieren\n",
    "DB_CONFIG = {\n",
    "    'host': os.getenv('DB_HOST', 'localhost'),\n",
    "    'database': os.getenv('DB_NAME', 'your_database'),\n",
    "    'user': os.getenv('DB_USER', 'your_username'),\n",
    "    'password': os.getenv('DB_PASSWORD', 'your_password'),\n",
    "    # Port ist optional, Standard-Port ist 3306\n",
    "    # 'port': os.getenv('DB_PORT', 3306),\n",
    "}\n",
    "\n",
    "def test_connection():\n",
    "    \"\"\"Testet die Verbindung zur MySQL-Datenbank.\"\"\"\n",
    "    connection = None\n",
    "    try:\n",
    "        # Verbindung zur Datenbank herstellen\n",
    "        connection = mysql.connector.connect(**DB_CONFIG)\n",
    "        \n",
    "        if connection.is_connected():\n",
    "            db_info = connection.get_server_info()\n",
    "            print(f\"‚úÖ Erfolgreich verbunden mit MySQL Server, Version: {db_info}\")\n",
    "            \n",
    "            # Datenbankname anzeigen\n",
    "            cursor = connection.cursor()\n",
    "            cursor.execute(\"SELECT DATABASE();\")\n",
    "            record = cursor.fetchone()\n",
    "            print(f\"   Verbunden mit Datenbank: {record[0]}\")\n",
    "            \n",
    "            # Tabellen auflisten\n",
    "            cursor.execute(\"SHOW TABLES;\")\n",
    "            tables = cursor.fetchall()\n",
    "            \n",
    "            if tables:\n",
    "                print(\"\\nüìã Verf√ºgbare Tabellen:\")\n",
    "                for table in tables:\n",
    "                    print(f\"   - {table[0]}\")\n",
    "                \n",
    "                # Optional: Zeige Spalten der ersten Tabelle\n",
    "                first_table = tables[0][0]\n",
    "                cursor.execute(f\"DESCRIBE {first_table};\")\n",
    "                columns = cursor.fetchall()\n",
    "                \n",
    "                print(f\"\\nüìä Struktur der Tabelle '{first_table}':\")\n",
    "                for column in columns:\n",
    "                    print(f\"   - {column[0]} ({column[1]})\")\n",
    "            else:\n",
    "                print(\"   Keine Tabellen in der Datenbank gefunden.\")\n",
    "                \n",
    "    except Error as e:\n",
    "        print(f\"‚ùå Fehler bei Verbindung zur MySQL-Datenbank: {e}\")\n",
    "    finally:\n",
    "        # Verbindung schlie√üen, falls offen\n",
    "        if connection and connection.is_connected():\n",
    "            if 'cursor' in locals():\n",
    "                cursor.close()\n",
    "            connection.close()\n",
    "            print(\"\\nüîí MySQL-Verbindung geschlossen.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üîÑ Teste MySQL-Verbindung...\")\n",
    "    \n",
    "    # √úberpr√ºfen, ob notwendige Umgebungsvariablen gesetzt sind\n",
    "    missing_vars = []\n",
    "    for key in ['DB_HOST', 'DB_NAME', 'DB_USER', 'DB_PASSWORD']:\n",
    "        if not os.getenv(key):\n",
    "            missing_vars.append(key)\n",
    "    \n",
    "    if missing_vars:\n",
    "        print(f\"‚ö†Ô∏è  Warnung: Folgende Umgebungsvariablen sind nicht gesetzt: {', '.join(missing_vars)}\")\n",
    "        print(\"   Verwende Standard-Werte aus dem Skript.\")\n",
    "        \n",
    "        # M√∂glichkeit, Werte manuell einzugeben\n",
    "        use_defaults = input(\"M√∂chtest du die Default-Werte verwenden? (j/n): \")\n",
    "        if use_defaults.lower() != 'j':\n",
    "            DB_CONFIG['host'] = input(\"DB_HOST: \") or DB_CONFIG['host']\n",
    "            DB_CONFIG['database'] = input(\"DB_NAME: \") or DB_CONFIG['database']\n",
    "            DB_CONFIG['user'] = input(\"DB_USER: \") or DB_CONFIG['user']\n",
    "            DB_CONFIG['password'] = input(\"DB_PASSWORD: \") or DB_CONFIG['password']\n",
    "    \n",
    "    test_connection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19c88e29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Loading JSON data from ../../jobs_all_processed.json...\n",
      "üîÑ Preprocessing data...\n",
      "\n",
      "üìä Performing EDA (Exploratory Data Analysis)...\n",
      "üìä Total number of records: 4438\n",
      "üîë Found fields: 31\n",
      "   added_at, application_deadline, application_submission, conditions_d_admission, contract_type, created_at, education_level, extracted_id, full_description, general_information, group_classification, how_to_apply, imported_at, job_category, job_details, link, location, ministry, missions, nationality, organization, profile, recruiter, required_documents, salary_group, source_file, status, task, title, updated_at, vacancy_count\n",
      "\n",
      "üìã Field statistics:\n",
      "Field                          Completeness    Types                          Max Length\n",
      "-------------------------------------------------------------------------------------\n",
      "added_at                       4438/4438 (100.0%) str:4438                       19        \n",
      "application_deadline           4438/4438 (100.0%) str:4438                       10        \n",
      "application_submission         9/4438 (0.2%)   str:9                          206       \n",
      "conditions_d_admission         329/4438 (7.4%) str:329                        1267      \n",
      "contract_type                  3180/4438 (71.7%) str:3180                       77        \n",
      "created_at                     4438/4438 (100.0%) str:4438                       26        \n",
      "education_level                4265/4438 (96.1%) str:4265                       60        \n",
      "extracted_id                   4374/4438 (98.6%) int:4374                       N/A       \n",
      "full_description               432/4438 (9.7%) str:432                        8838      \n",
      "general_information            432/4438 (9.7%) str:432                        481       \n",
      "group_classification           4398/4438 (99.1%) str:4398                       81        \n",
      "how_to_apply                   47/4438 (1.1%)  str:47                         687       \n",
      "imported_at                    4438/4438 (100.0%) str:4438                       19        \n",
      "job_category                   3807/4438 (85.8%) str:3807                       51        \n",
      "job_details                    102/4438 (2.3%) str:102                        324       \n",
      "link                           4438/4438 (100.0%) str:4438                       160       \n",
      "location                       631/4438 (14.2%) str:631                        36        \n",
      "ministry                       3708/4438 (83.6%) str:3708                       107       \n",
      "missions                       330/4438 (7.4%) str:330                        4249      \n",
      "nationality                    4282/4438 (96.5%) str:4282                       36        \n",
      "organization                   3807/4438 (85.8%) str:3807                       129       \n",
      "profile                        330/4438 (7.4%) str:330                        2904      \n",
      "recruiter                      330/4438 (7.4%) str:330                        2330      \n",
      "required_documents             314/4438 (7.1%) str:314                        937       \n",
      "salary_group                   44/4438 (1.0%)  str:44                         0         \n",
      "source_file                    4438/4438 (100.0%) str:4438                       23        \n",
      "status                         4435/4438 (99.9%) str:4435                       35        \n",
      "task                           4411/4438 (99.4%) str:4411                       27        \n",
      "title                          4438/4438 (100.0%) str:4438                       179       \n",
      "updated_at                     414/4438 (9.3%) str:414                        26        \n",
      "vacancy_count                  4432/4438 (99.9%) str:4432                       2         \n",
      "\n",
      "üîÑ Connecting to MySQL database...\n",
      "‚úÖ Table 'jobs' already exists, will use it for import\n",
      "üîÑ Preparing to import 4438 records...\n",
      "üìä Found 4438 existing records in database\n",
      "‚úÖ 0 new records imported, 4438 duplicates skipped\n",
      "‚úÖ Unique index on link ensured\n",
      "üéâ Import completed!\n",
      "\n",
      "üìã Structure of the created table 'jobs':\n",
      "   - id (int(11))\n",
      "   - added_at (datetime)\n",
      "   - application_deadline (varchar(255))\n",
      "   - application_submission (varchar(255))\n",
      "   - conditions_d_admission (varchar(255))\n",
      "   - contract_type (varchar(255))\n",
      "   - created_at (datetime)\n",
      "   - education_level (varchar(255))\n",
      "   - extracted_id (int(11))\n",
      "   - full_description (text)\n",
      "   - general_information (text)\n",
      "   - group_classification (varchar(255))\n",
      "   - how_to_apply (text)\n",
      "   - imported_at (datetime)\n",
      "   - job_category (varchar(255))\n",
      "   - job_details (text)\n",
      "   - link (varchar(255))\n",
      "   - location (varchar(255))\n",
      "   - ministry (varchar(255))\n",
      "   - missions (text)\n",
      "   - nationality (varchar(255))\n",
      "   - organization (varchar(255))\n",
      "   - profile (text)\n",
      "   - recruiter (text)\n",
      "   - required_documents (text)\n",
      "   - salary_group (varchar(255))\n",
      "   - source_file (varchar(255))\n",
      "   - status (varchar(255))\n",
      "   - task (varchar(255))\n",
      "   - title (varchar(255))\n",
      "   - updated_at (datetime)\n",
      "   - vacancy_count (int(11))\n",
      "\n",
      "üí° Recommended indexes:\n",
      "   - CREATE INDEX idx_organization ON jobs(organization);\n",
      "   - CREATE INDEX idx_location ON jobs(location);\n",
      "   - CREATE INDEX idx_job_category ON jobs(job_category);\n",
      "   - CREATE INDEX idx_created_at ON jobs(created_at);\n",
      "   - CREATE INDEX idx_added_at ON jobs(added_at);\n",
      "\n",
      "üìä Sample queries:\n",
      "   - SELECT COUNT(*) FROM jobs;\n",
      "   - SELECT * FROM jobs WHERE link LIKE '%313905.html';\n",
      "   - SELECT * FROM jobs WHERE added_at > '2025-04-01' LIMIT 10;\n",
      "   - SELECT organization, COUNT(*) FROM jobs GROUP BY organization ORDER BY COUNT(*) DESC LIMIT 10;\n",
      "üîí Database connection closed\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "JSON EDA and MySQL Importer\n",
    "---------------------------\n",
    "First performs an exploratory data analysis and then imports data into MySQL\n",
    "with improved field naming and lineage tracking.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import re\n",
    "import pymysql\n",
    "from pymysql.cursors import DictCursor\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "\n",
    "# Configuration\n",
    "DB_CONFIG = {\n",
    "    'host': os.getenv('DB_HOST', 'db.riespatrick.de'),\n",
    "    'database': os.getenv('DB_NAME', 'deine_datenbank'),\n",
    "    'user': os.getenv('DB_USER', 'testjobs'),\n",
    "    'password': os.getenv('DB_PASSWORD', 'dein_passwort'),\n",
    "    'charset': 'utf8mb4',\n",
    "    'cursorclass': DictCursor\n",
    "}\n",
    "\n",
    "# JSON file\n",
    "JSON_FILE = '../../jobs_all_processed.json'\n",
    "TABLE_NAME = 'jobs'  # Name of the table to be created\n",
    "\n",
    "# Field mapping to English-friendly names\n",
    "FIELD_MAPPING = {\n",
    "    'Administration/Organization': 'organization',\n",
    "    'Application Deadline': 'application_deadline',\n",
    "    'Conditions d\\'admission': 'admission_conditions',\n",
    "    'Contract Type': 'contract_type',\n",
    "    'Documents √† fournir': 'required_documents',\n",
    "    'D√©pot de candidature': 'application_submission',\n",
    "    'D√©tail du poste': 'job_details',\n",
    "    'Education Level': 'education_level',\n",
    "    'Full Description': 'full_description',\n",
    "    'Group Classification': 'group_classification',\n",
    "    'Informations g√©n√©rales': 'general_information',\n",
    "    'Job Category': 'job_category',\n",
    "    'Link': 'link',\n",
    "    'Location': 'location',\n",
    "    'Ministry': 'ministry',\n",
    "    'Missions': 'missions',\n",
    "    'Nationality': 'nationality',\n",
    "    'Number of Vacancies': 'vacancy_count',\n",
    "    'Postuler': 'how_to_apply',\n",
    "    'Profil': 'profile',\n",
    "    'Qui recrute ?': 'recruiter',\n",
    "    'Salary Group': 'salary_group',\n",
    "    'Status': 'status',\n",
    "    'Task': 'task',\n",
    "    'Title': 'title',\n",
    "    'adding_date': 'created_at',\n",
    "    'updated_date': 'updated_at'\n",
    "}\n",
    "\n",
    "# Fields that should always be treated as TEXT\n",
    "TEXT_FIELDS = [\n",
    "    'admission_conditions', 'required_documents', 'job_details', 'full_description',\n",
    "    'general_information', 'missions', 'how_to_apply', 'profile', 'recruiter'\n",
    "]\n",
    "\n",
    "# Fields that should be used in the unique constraint for duplicate detection\n",
    "UNIQUE_FIELDS = ['link']\n",
    "\n",
    "def load_json_data(file_path):\n",
    "    \"\"\"Loads data from the JSON file.\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            return json.load(file)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ùå Error: File '{file_path}' not found.\")\n",
    "        sys.exit(1)\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"‚ùå Error: File '{file_path}' contains invalid JSON.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "def extract_job_id_from_url(url):\n",
    "    \"\"\"Extracts the unique ID from the job URL.\"\"\"\n",
    "    # Pattern to match the numerical ID at the end of the URL\n",
    "    pattern = r'-(\\d+)\\.html$'\n",
    "    match = re.search(pattern, url)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    return None\n",
    "\n",
    "def preprocess_data(data):\n",
    "    \"\"\"Preprocesses the data, adding job_id from URL and mapping field names.\"\"\"\n",
    "    processed_data = []\n",
    "    current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    for item in data:\n",
    "        processed_item = {}\n",
    "        \n",
    "        # Extract job_id from Link field if available, but don't rely on it for uniqueness\n",
    "        if 'Link' in item:\n",
    "            job_id = extract_job_id_from_url(item['Link'])\n",
    "            if job_id:\n",
    "                processed_item['extracted_id'] = job_id\n",
    "        \n",
    "        # Map old field names to new ones\n",
    "        for old_name, value in item.items():\n",
    "            # Use the mapping if available, otherwise sanitize the field name\n",
    "            if old_name in FIELD_MAPPING:\n",
    "                new_name = FIELD_MAPPING[old_name]\n",
    "            else:\n",
    "                # Sanitize field name by replacing special characters and spaces with underscores\n",
    "                new_name = re.sub(r'[^a-zA-Z0-9]', '_', old_name).lower()\n",
    "                # Ensure no double underscores\n",
    "                new_name = re.sub(r'_+', '_', new_name)\n",
    "                # Remove leading/trailing underscores\n",
    "                new_name = new_name.strip('_')\n",
    "                \n",
    "            processed_item[new_name] = value\n",
    "        \n",
    "        # Add lineage information\n",
    "        processed_item['source_file'] = os.path.basename(JSON_FILE)\n",
    "        processed_item['imported_at'] = current_time\n",
    "        processed_item['added_at'] = current_time  # When the record was added to the database\n",
    "        \n",
    "        processed_data.append(processed_item)\n",
    "    \n",
    "    return processed_data\n",
    "\n",
    "def analyze_json_structure(data):\n",
    "    \"\"\"Analyzes the structure of the JSON data.\"\"\"\n",
    "    if not isinstance(data, list):\n",
    "        print(\"‚ö†Ô∏è JSON data is not an array. Converting to an array...\")\n",
    "        data = [data]\n",
    "    \n",
    "    total_records = len(data)\n",
    "    print(f\"üìä Total number of records: {total_records}\")\n",
    "    \n",
    "    # Collect all unique keys\n",
    "    all_keys = set()\n",
    "    for item in data:\n",
    "        all_keys.update(item.keys())\n",
    "    \n",
    "    print(f\"üîë Found fields: {len(all_keys)}\")\n",
    "    print(\"   \" + \", \".join(sorted(all_keys)))\n",
    "    \n",
    "    # Analyze how many records have each field\n",
    "    field_stats = {}\n",
    "    value_types = {}\n",
    "    max_lengths = {}\n",
    "    \n",
    "    for key in all_keys:\n",
    "        field_count = sum(1 for item in data if key in item)\n",
    "        field_stats[key] = field_count\n",
    "        \n",
    "        # Analyze data types\n",
    "        types_counter = Counter()\n",
    "        lengths = []\n",
    "        \n",
    "        for item in data:\n",
    "            if key in item:\n",
    "                value = item[key]\n",
    "                types_counter[type(value).__name__] += 1\n",
    "                \n",
    "                # Measure length for strings\n",
    "                if isinstance(value, str):\n",
    "                    lengths.append(len(value))\n",
    "        \n",
    "        value_types[key] = dict(types_counter)\n",
    "        \n",
    "        if lengths:\n",
    "            max_lengths[key] = max(lengths)\n",
    "    \n",
    "    # Output results\n",
    "    print(\"\\nüìã Field statistics:\")\n",
    "    print(f\"{'Field':<30} {'Completeness':<15} {'Types':<30} {'Max Length':<10}\")\n",
    "    print(\"-\" * 85)\n",
    "    \n",
    "    for key in sorted(all_keys):\n",
    "        completeness = f\"{field_stats[key]}/{total_records} ({field_stats[key]/total_records*100:.1f}%)\"\n",
    "        types = ', '.join(f\"{t}:{c}\" for t, c in value_types[key].items())\n",
    "        max_len = max_lengths.get(key, 'N/A')\n",
    "        \n",
    "        print(f\"{key:<30} {completeness:<15} {types:<30} {max_len:<10}\")\n",
    "    \n",
    "    return {\n",
    "        'total_records': total_records,\n",
    "        'all_keys': all_keys,\n",
    "        'field_stats': field_stats,\n",
    "        'value_types': value_types,\n",
    "        'max_lengths': max_lengths\n",
    "    }\n",
    "\n",
    "def determine_column_type(key):\n",
    "    \"\"\"Determines the optimal MySQL data type based on the field name.\"\"\"\n",
    "    # Check if it's a text field\n",
    "    if key in TEXT_FIELDS:\n",
    "        return 'TEXT'\n",
    "    \n",
    "    # Special cases\n",
    "    if key == 'extracted_id':\n",
    "        return 'INT'\n",
    "    elif key == 'link':\n",
    "        return 'VARCHAR(255) UNIQUE'  # Make link unique\n",
    "    elif key in ['imported_at', 'created_at', 'updated_at', 'added_at']:\n",
    "        return 'DATETIME'\n",
    "    elif key == 'vacancy_count':\n",
    "        return 'INT'\n",
    "    \n",
    "    # Default to VARCHAR(255) for other fields\n",
    "    return 'VARCHAR(255)'\n",
    "\n",
    "def create_table_with_all_columns(connection, data, analysis, table_name):\n",
    "    \"\"\"Creates a table with all found columns.\"\"\"\n",
    "    columns = []\n",
    "    \n",
    "    # ID column as primary key\n",
    "    columns.append(\"`id` INT AUTO_INCREMENT PRIMARY KEY\")\n",
    "    \n",
    "    # All other columns based on the analysis\n",
    "    for key in sorted(analysis['all_keys']):\n",
    "        if key.lower() == 'id':  # Skip if already added\n",
    "            continue\n",
    "            \n",
    "        col_type = determine_column_type(key)\n",
    "        \n",
    "        # Escape for MySQL column names - handle special characters\n",
    "        escaped_key = f\"`{key}`\"\n",
    "        columns.append(f\"{escaped_key} {col_type}\")\n",
    "    \n",
    "    # Add lineage columns\n",
    "    if 'source_file' not in analysis['all_keys']:\n",
    "        columns.append(\"`source_file` VARCHAR(255)\")\n",
    "    if 'imported_at' not in analysis['all_keys']:\n",
    "        columns.append(\"`imported_at` DATETIME\")\n",
    "    if 'added_at' not in analysis['all_keys']:\n",
    "        columns.append(\"`added_at` DATETIME\")\n",
    "    \n",
    "    with connection.cursor() as cursor:\n",
    "        # Check if table already exists\n",
    "        cursor.execute(f\"SHOW TABLES LIKE '{table_name}'\")\n",
    "        table_exists = cursor.fetchone()\n",
    "        \n",
    "        # If table doesn't exist, create it\n",
    "        if not table_exists:\n",
    "            create_table_sql = f\"CREATE TABLE `{table_name}` ({', '.join(columns)})\"\n",
    "            print(f\"üîß Creating table with SQL command:\\n{create_table_sql}\")\n",
    "            cursor.execute(create_table_sql)\n",
    "            print(f\"‚úÖ Table '{table_name}' successfully created\")\n",
    "        else:\n",
    "            print(f\"‚úÖ Table '{table_name}' already exists, will use it for import\")\n",
    "            \n",
    "            # Check if we need to add any missing columns\n",
    "            cursor.execute(f\"DESCRIBE `{table_name}`\")\n",
    "            existing_columns = [col['Field'].lower() for col in cursor.fetchall()]\n",
    "            \n",
    "            for column_def in columns:\n",
    "                column_parts = column_def.split()\n",
    "                column_name = column_parts[0].replace('`', '')\n",
    "                if column_name.lower() not in existing_columns and column_name.lower() != 'id':\n",
    "                    try:\n",
    "                        add_column_sql = f\"ALTER TABLE `{table_name}` ADD COLUMN {column_def}\"\n",
    "                        print(f\"üîß Adding missing column: {column_name}\")\n",
    "                        cursor.execute(add_column_sql)\n",
    "                    except pymysql.err.OperationalError as e:\n",
    "                        if \"Duplicate column name\" in str(e):\n",
    "                            print(f\"‚ö†Ô∏è Column {column_name} already exists with a different case\")\n",
    "                        else:\n",
    "                            print(f\"‚ö†Ô∏è Error adding column {column_name}: {e}\")\n",
    "        \n",
    "        # Insert data in batches with duplicate handling\n",
    "        inserted_count = 0\n",
    "        skipped_count = 0\n",
    "        batch_size = 50  # Lower batch size to reduce memory usage\n",
    "        current_batch = []\n",
    "        \n",
    "        print(f\"üîÑ Preparing to import {len(data)} records...\")\n",
    "        \n",
    "        # First, collect existing links to avoid unnecessary insertion attempts\n",
    "        existing_links = set()\n",
    "        if 'link' in analysis['all_keys']:\n",
    "            try:\n",
    "                cursor.execute(f\"SELECT link FROM `{table_name}` WHERE link IS NOT NULL\")\n",
    "                results = cursor.fetchall()\n",
    "                existing_links = {row['link'] for row in results if row['link']}\n",
    "                print(f\"üìä Found {len(existing_links)} existing records in database\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Could not fetch existing links: {e}\")\n",
    "                # Continue anyway - we'll rely on the database's UNIQUE constraint\n",
    "        \n",
    "        for item in data:\n",
    "            # Skip if link already exists in database\n",
    "            if 'link' in item and item['link'] in existing_links:\n",
    "                skipped_count += 1\n",
    "                continue\n",
    "                \n",
    "            # Only columns that exist in the current record\n",
    "            item_columns = [key for key in item.keys() if key.lower() != 'id']\n",
    "            \n",
    "            if not item_columns:\n",
    "                continue  # Skip if no valid columns are present\n",
    "            \n",
    "            # Prepare the values\n",
    "            values = []\n",
    "            for col in item_columns:\n",
    "                value = item[col]\n",
    "                # Serialize JSON data\n",
    "                if isinstance(value, (dict, list)):\n",
    "                    value = json.dumps(value)\n",
    "                values.append(value)\n",
    "            \n",
    "            # Add to current batch\n",
    "            current_batch.append((item_columns, values))\n",
    "            \n",
    "            # Process batch if it reaches the batch size\n",
    "            if len(current_batch) >= batch_size:\n",
    "                process_batch(cursor, table_name, current_batch)\n",
    "                inserted_count += len(current_batch)\n",
    "                current_batch = []\n",
    "                \n",
    "                # Show status every 200 records\n",
    "                if inserted_count % 200 == 0:\n",
    "                    print(f\"   {inserted_count}/{len(data) - skipped_count} records imported...\")\n",
    "                    connection.commit()  # Commit periodically to avoid large transactions\n",
    "        \n",
    "        # Process remaining records\n",
    "        if current_batch:\n",
    "            process_batch(cursor, table_name, current_batch)\n",
    "            inserted_count += len(current_batch)\n",
    "        \n",
    "        connection.commit()\n",
    "        print(f\"‚úÖ {inserted_count} new records imported, {skipped_count} duplicates skipped\")\n",
    "        \n",
    "        # Ensure we have an index on the link field for better performance\n",
    "        try:\n",
    "            cursor.execute(f\"CREATE UNIQUE INDEX IF NOT EXISTS idx_link ON `{table_name}` (link)\")\n",
    "            print(\"‚úÖ Unique index on link ensured\")\n",
    "        except pymysql.err.InternalError as e:\n",
    "            if \"Duplicate key name\" not in str(e):\n",
    "                raise\n",
    "            print(\"‚úÖ Unique index on link already exists\")\n",
    "        # Older MySQL versions don't support IF NOT EXISTS for indexes\n",
    "        except pymysql.err.OperationalError:\n",
    "            try:\n",
    "                cursor.execute(f\"SHOW INDEX FROM `{table_name}` WHERE Column_name = 'link'\")\n",
    "                if not cursor.fetchone():\n",
    "                    cursor.execute(f\"CREATE UNIQUE INDEX idx_link ON `{table_name}` (link)\")\n",
    "                    print(\"‚úÖ Unique index on link created\")\n",
    "                else:\n",
    "                    print(\"‚úÖ Unique index on link already exists\")\n",
    "            except Exception as idx_err:\n",
    "                print(f\"‚ö†Ô∏è Note: Could not verify or create index on link: {idx_err}\")\n",
    "        \n",
    "        return True\n",
    "\n",
    "def process_batch(cursor, table_name, batch):\n",
    "    \"\"\"Process a batch of records with INSERT IGNORE to handle duplicates.\"\"\"\n",
    "    for item_columns, values in batch:\n",
    "        placeholders = ', '.join(['%s'] * len(item_columns))\n",
    "        \n",
    "        # Use INSERT IGNORE to skip duplicates based on unique constraints\n",
    "        insert_sql = f\"INSERT IGNORE INTO `{table_name}` (`{'`, `'.join(item_columns)}`) VALUES ({placeholders})\"\n",
    "        \n",
    "        try:\n",
    "            cursor.execute(insert_sql, values)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error inserting a record: {e}\")\n",
    "            # Continue despite error\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run the analysis and import.\"\"\"\n",
    "    print(f\"üîÑ Loading JSON data from {JSON_FILE}...\")\n",
    "    raw_data = load_json_data(JSON_FILE)\n",
    "    \n",
    "    print(f\"üîÑ Preprocessing data...\")\n",
    "    data = preprocess_data(raw_data)\n",
    "    \n",
    "    print(\"\\nüìä Performing EDA (Exploratory Data Analysis)...\")\n",
    "    analysis = analyze_json_structure(data)\n",
    "    \n",
    "    try:\n",
    "        # Configure database connection with optimized settings\n",
    "        config = DB_CONFIG.copy()\n",
    "        config['connect_timeout'] = 30  # Increase timeout for large datasets\n",
    "        config['autocommit'] = False  # We'll handle transactions manually\n",
    "        \n",
    "        print(f\"\\nüîÑ Connecting to MySQL database...\")\n",
    "        connection = pymysql.connect(**config)\n",
    "        \n",
    "        proceed = input(\"\\n‚ö†Ô∏è Do you want to proceed with the import? (y/n): \")\n",
    "        if proceed.lower() != 'y':\n",
    "            print(\"Import aborted.\")\n",
    "            return\n",
    "        \n",
    "        # Set longer timeout for operations\n",
    "        with connection.cursor() as cursor:\n",
    "            cursor.execute(\"SET SESSION wait_timeout = 28800\")  # 8 hours\n",
    "            # Note: max_allowed_packet is a global setting and can't be changed at session level\n",
    "        \n",
    "        success = create_table_with_all_columns(connection, data, analysis, TABLE_NAME)\n",
    "        \n",
    "        if success:\n",
    "            print(f\"üéâ Import completed!\")\n",
    "            \n",
    "            # Show structure of the created table\n",
    "            with connection.cursor() as cursor:\n",
    "                cursor.execute(f\"DESCRIBE `{TABLE_NAME}`\")\n",
    "                columns = cursor.fetchall()\n",
    "                \n",
    "                print(f\"\\nüìã Structure of the created table '{TABLE_NAME}':\")\n",
    "                for column in columns:\n",
    "                    print(f\"   - {column['Field']} ({column['Type']})\")\n",
    "                \n",
    "                # Index recommendations\n",
    "                print(\"\\nüí° Recommended indexes:\")\n",
    "                print(\"   - CREATE INDEX idx_organization ON jobs(organization);\")\n",
    "                print(\"   - CREATE INDEX idx_location ON jobs(location);\")\n",
    "                print(\"   - CREATE INDEX idx_job_category ON jobs(job_category);\")\n",
    "                print(\"   - CREATE INDEX idx_created_at ON jobs(created_at);\")\n",
    "                print(\"   - CREATE INDEX idx_added_at ON jobs(added_at);\")\n",
    "                \n",
    "                # Show usage example\n",
    "                print(\"\\nüìä Sample queries:\")\n",
    "                print(\"   - SELECT COUNT(*) FROM jobs;\")\n",
    "                print(\"   - SELECT * FROM jobs WHERE link LIKE '%313905.html';\")\n",
    "                print(\"   - SELECT * FROM jobs WHERE added_at > '2025-04-01' LIMIT 10;\")\n",
    "                print(\"   - SELECT organization, COUNT(*) FROM jobs GROUP BY organization ORDER BY COUNT(*) DESC LIMIT 10;\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "    finally:\n",
    "        if 'connection' in locals() and connection.open:\n",
    "            connection.close()\n",
    "            print(\"üîí Database connection closed\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
